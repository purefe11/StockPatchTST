# Transformer ë‚´ë¶€ êµ¬ì¡° (ì‰¬ìš´ ë²„ì „!)
> TransformerëŠ” 'ì—¬ëŸ¬ ì¹œêµ¬ê°€ í•¨ê»˜ íšŒì˜í•˜ë©´ì„œ, ëˆ„ê°€ ì¤‘ìš”í•œ ë§ì„ í–ˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” êµ¬ì¡°'ë¡œ ì´í•´í•˜ë©´ ì‰½ë‹¤.
<img src=https://github.com/user-attachments/assets/72a1f54d-dfdf-49bf-8328-c2fff4ce4043 width=640/>

> 30ì¼ ë™ì•ˆì˜ ì£¼ì‹ ë°ì´í„°ë¥¼ 9ê°œ ì¡°ê°(íŒ¨ì¹˜)ìœ¼ë¡œ ì˜ëë‹¤ê³  í–ˆì„ ë•Œ
> ê° ì¡°ê°ì€ 6ì¼ ë™ì•ˆì˜ ì£¼ê°€ íë¦„ì„ ë‹´ê³  ìˆë‹¤.
> ì´ì œ Transformerì—ì„œ ì´ 9ê°œì˜ ì¡°ê°ì—ê²Œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì‚´í´ë³´ì.

---

## ğŸ§© 1. Multi-head Self-Attention  
**â€œê° ì¡°ê°ì´ ë‹¤ë¥¸ ì¡°ê°ë“¤ì„ ë°”ë¼ë³´ë©´ì„œ ì¤‘ìš”ë„ë¥¼ ì •í•œë‹¤.â€**

- í¼ì¦ ì¡°ê°ë“¤(íŒ¨ì¹˜)ë¼ë¦¬ ì„œë¡œë¥¼ ë°”ë¼ë³¸ë‹¤.  
- "ë„ˆëŠ” ë‚˜ì™€ ê´€ë ¨ ìˆì–´!", "ë„Œ ëœ ì¤‘ìš”í•´" ê°™ì€ íŒë‹¨ì„ ì—¬ëŸ¬ ë°©í–¥ì—ì„œ í•œë‹¤.  
- ì—¬ëŸ¬ ê°œì˜ ì‹œì„ ìœ¼ë¡œ ë³¸ë‹¤ê³  í•´ì„œ **Multi-head (ì—¬ëŸ¬ ê°œì˜ ëˆˆ)** ì´ë¼ê³  í•œë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> ì¹œêµ¬ë“¤ê³¼ ëª¨ì—¬ì„œ, "ìš°ë¦¬ ì¤‘ ëˆ„ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì´ì•¼ê¸°ë¥¼ í–ˆëŠ”ì§€" ì—¬ëŸ¬ ì‹œì„ ì—ì„œ ìƒê°í•´ë³´ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤.

---

## â• 2. Add & LayerNorm  
**â€œìì‹ ì˜ ìƒê°ê³¼ ì¹œêµ¬ë“¤ ì˜ê²¬ì„ í•©ì³ì„œ ì •ë¦¬í•œë‹¤.â€**

- ì¹œêµ¬ë“¤ì˜ ì˜ê²¬(Attention ê²°ê³¼)ê³¼ ì›ë˜ ìê¸° ìƒê°ì„ ë”í•œë‹¤.  
- ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ê°€ ë„ˆë¬´ íŠ€ì§€ ì•Šë„ë¡ ì •ë¦¬í•œë‹¤. (= LayerNorm)

ğŸ’¡ *ë¹„ìœ :*  
> ì¹œêµ¬ ì˜ê²¬ + ë‚´ ì˜ê²¬ = ë” ë˜‘ë˜‘í•´ì§„ ë‚˜!

---

## ğŸ”§ 3. Feed Forward (ì„ í˜• â†’ ReLU â†’ ì„ í˜•)  
**â€œìƒê°ì„ ë” ê¹Šê²Œ ì •ë¦¬í•´ì„œ ë‹¤ì‹œ í‘œí˜„í•œë‹¤.â€**

- ë‘ ë²ˆì˜ ê³„ì‚°ì„ í†µí•´(ìˆ«ì ëŠ˜ë ¸ë‹¤ê°€ ì¤„ì´ëŠ” ê³¼ì •) ìƒê°ì„ ì •ë¦¬í•˜ê³  ë‹¤ë“¬ëŠ”ë‹¤.  
- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)ëŠ” "ìŒìˆ˜ëŠ” ì‹«ì–´!"ë¼ê³  ë§í•˜ë©´ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸´ë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> í•œ ë²ˆ ë” ìƒê°í•´ì„œ ë” ë©‹ì§„ ë¬¸ì¥ì„ ë§Œë“œëŠ” ê³¼ì •ê³¼ ê°™ë‹¤.

---

## â• 4. Add & LayerNorm (ë˜ ë‚˜ì˜¨ë‹¤!)  
**â€œì´ë²ˆì—ë„ ë‚´ ìƒê°ê³¼ ìƒˆë¡œ ì •ë¦¬í•œ ìƒê°ì„ í•©ì³ì„œ ë‹¤ì‹œ ì •ë¦¬í•œë‹¤.â€**

- ì •ë¦¬ëœ ìƒê° + ì›ë˜ ìƒê° â†’ í•œ ë²ˆ ë” ì •ë¦¬!
- ì´ëŸ° ê³¼ì •ì„ ê²¹ê²¹ì´ ìŒ“ìœ¼ë©´ì„œ ë” ë˜‘ë˜‘í•´ì§„ë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> ë‚´ê°€ í•œ ë§ + ìƒˆë¡œ ì •ë¦¬í•œ ë§ = ì™„ì„±ëœ ë©‹ì§„ ë§!

---

## ğŸ‰ Transformerê°€ ë­˜ í•´ì£¼ëŠ” ê±¸ê¹Œ?

- 9ê°œì˜ ì¡°ê°ì„ ì˜ ì´í•´í–ˆê³ (ì¡°ê°ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì˜ íŒŒì•…í•˜ê³ ),
- ì–´ë–¤ ì¡°ê°ì´ ì¤‘ìš”í•œì§€ë„ ì•Œì•„ë‚¸ë‹¤.  
- â†’ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ì¤€ë¹„ ì™„ë£Œ!

---

## âœ¨ í•œ ì¤„ ìš”ì•½

> **TransformerëŠ” ì—¬ëŸ¬ ì¹œêµ¬(ì¡°ê°)ê°€ ì„œë¡œì˜ ë§ì„ ë“¤ìœ¼ë©°,
> ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì°¾ì•„ë‚´ê³  ë˜‘ë˜‘í•˜ê²Œ ì •ë¦¬í•´ ì˜ˆì¸¡í•˜ëŠ” êµ¬ì¡°ë‹¤!**

<br>

# ğŸ“˜ Transformer ì¸ì½”ë” (ì‹¬í™” ë²„ì „!)
> âš ï¸ ë³¸ ì„¹ì…˜ì—ëŠ” ì•„ë˜ ì¶œì²˜ì˜ ë„ì‹ ì´ë¯¸ì§€ ì¼ë¶€ë¥¼ ì¸ìš© ë˜ëŠ” ì¬êµ¬ì„±í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

- ["Attention is All You Need"](https://arxiv.org/abs/1706.03762)
- [Seq2seq Models With Attention](https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [The Illustrated Transformer by Jay Alammar](https://nlpinkorean.github.io/illustrated-transformer/)

---

### ğŸ§© 1. Query, Key, Value ìƒì„±

![QKV](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;Q%20%3D%20XW%5EQ%2C%5Cquad%20K%20%3D%20XW%5EK%2C%5Cquad%20V%20%3D%20XW%5EV)

![transformer_self_attention_vectors](https://github.com/user-attachments/assets/83db602c-417f-4982-a29d-a5a05bf3bf43)
![self-attention-matrix-calculation](https://github.com/user-attachments/assets/45aa8662-704b-426a-8b44-1b89c6a74637)

ğŸ‘‰ ì„¤ëª…:
- \( X \): ì…ë ¥ ì‹œí€€ìŠ¤ (nê°œì˜ ë²¡í„°, ê° ë²¡í„° ì°¨ì› model_dim)
- ê°ê°ì— ëŒ€í•´ **Query (ì§ˆë¬¸), Key (ì •ë³´ì˜ ìœ„ì¹˜), Value (ì‹¤ì œ ì •ë³´)** ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì„ í˜• ë³€í™˜ 
- \( W^Q, W^K, W^V \): Query, Key, Value ìƒì„±ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬

ğŸ“Œ í•µì‹¬ ê°œë…:
- ê° ì…ë ¥ ë²¡í„°ë¥¼ ì§ˆë¬¸, ì •ë³´ ìƒ‰ì¸, ì •ë³´ ë‚´ìš©ì˜ ì„¸ ê°€ì§€ë¡œ ë‚˜ëˆ ì„œ ì¤€ë¹„í•¨

ğŸ’¡ *ë¹„ìœ :*
- ë¬¸ì¥: "ë‚˜ëŠ” Transformerë¥¼ ê³µë¶€í•œë‹¤"
  - Q(Query): "ê³µë¶€í•œë‹¤"ë¼ëŠ” ë‹¨ì–´ì—ì„œ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì§ˆë¬¸
  - K(Key): "ë‚˜ëŠ”", "Transformer", "ë¥¼", ë“± ê° ë‹¨ì–´ì˜ ì •ë³´ ìš”ì•½ â€” ë‚˜ì™€ ê´€ë ¨ ìˆëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€
  - V(Value): "ë‚˜ëŠ”", "Transformer", "ë¥¼", ë“±ì˜ ì‹¤ì œ ì˜ë¯¸ ë²¡í„° â€” ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©í•  ê°’
- ğŸ“Œ ì•„ë˜ Attention ê³¼ì •ì„ í†µí•´ "ê³µë¶€í•œë‹¤"ë¼ëŠ” ë‹¨ì–´ëŠ” ìì‹ (Q)ì´ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨í•œ ë‹¨ì–´(K)ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë¥¼(V) ê°€ì ¸ì˜¤ê²Œ ëœë‹¤.

---

### 2. Scaled Dot-Product Attention

![Attention](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;\text{Attention}(Q%2C%20K%2C%20V)%20%3D%20\text{softmax}\left(\frac{QK%5ET}{\sqrt{d_k}}\right)V)

![self-attention-output](https://github.com/user-attachments/assets/daf6360a-4ce6-4a0a-a80b-852f26bf0a72)<br>
![self-attention-matrix-calculation-2](https://github.com/user-attachments/assets/128ee31d-a2b3-4ffd-9fa8-c1eea4873345)

ğŸ‘‰ ì„¤ëª…:
- QK^T: Queryì™€ Keyì˜ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ ê³„ì‚°
- 1/sqrt(dk): ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ìŠ¤ì¼€ì¼ ì¡°ì •
- softmax: 0~1 ë²”ìœ„ì˜ ê°€ì¤‘ì¹˜ë¡œ ë°”ê¿”ì„œ ì¤‘ìš”ë„ë¥¼ í™•ë¥ ì²˜ëŸ¼ í‘œí˜„
- ìœ„ ê²°ê³¼(Attention Score)ì— ğ‘‰ë¥¼ ê°€ì¤‘í•©(weighted sum)í•œë‹¤.

ğŸ“Œ í•µì‹¬ ê°œë…:
- ì…ë ¥ ì‹œí€€ìŠ¤ ê°„ ìœ ì‚¬ë„ì— ë”°ë¼ Valueë¥¼ ê°€ì¤‘ í‰ê· í•´ ì£¼ëŠ” ì—°ì‚°
- (ê° ìœ„ì¹˜ì˜ ì…ë ¥ ë²¡í„°ê°€, ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ìŠ¤ìº”í•˜ë©° ì–´ë–¤ ì •ë³´ë¥¼ ë” ê°€ì ¸ì˜¬ì§€ ê²°ì •í•˜ëŠ” ê³¼ì •)

---

### 3. Multi-head Attention

![MultiHead](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;\text{MultiHead}(X)%20%3D%20\text{Concat}(\text{head}_1%2C%20\dots%2C%20\text{head}_h)W^O)

![transformer_multi-headed_self-attention-recap](https://github.com/user-attachments/assets/9864cf84-ed63-41f3-acb5-3b711aa94097)

ğŸ‘‰ ì„¤ëª…:
- ì•ì—ì„œ ê³„ì‚°í•œ Attentionì„ ì—¬ëŸ¬ ê°œ ë³‘ë ¬ë¡œ ìˆ˜í–‰
â†’ ê° headê°€ ì„œë„ ë‹¤ë¥¸ ê´€ì (ì˜ˆ: ë‹¨ê¸° íë¦„, ì¥ê¸° íë¦„)ì—ì„œ Attentionì„ ìˆ˜í–‰í•˜ê³ 
- ë§ˆì§€ë§‰ì— Concatí•´ì„œ í•˜ë‚˜ë¡œ ì´ì–´ë¶™ì¸ ë‹¤ìŒ, W^Oë¡œ ë‹¤ì‹œ projection

ğŸ“Œ í•µì‹¬ ê°œë…:
- ì—¬ëŸ¬ ê°œì˜ ì‹œì„ ìœ¼ë¡œ ì…ë ¥ì„ ë°”ë¼ë³´ê³ , ì´ë“¤ì„ ì¢…í•©í•´ ë” í’ë¶€í•œ ì •ë³´ë¡œ ë³€í™˜

ğŸ“¦ [ì½”ë“œ ì˜ˆì‹œ: PyTorch `TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)
- ì´ ë ˆì´ì–´ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ multi-head attentionê³¼ feedforward blockì„ ëª¨ë‘ í¬í•¨í•˜ê³  ìˆìŒ
- ì¦‰, Transformer ì¸ì½”ë” ë¸”ë¡ ì „ì²´ êµ¬ì¡°ë¥¼ í•œ ì¤„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

---

### 4. Add & LayerNorm

![AddNorm1](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;Z_1%20%3D%20\text{LayerNorm}(X%20+%20\text{MultiHead}(X)))

![transformer_resideual_layer_norm_2](https://github.com/user-attachments/assets/8e2a0fb0-ec50-4f81-b565-c72ec3098667)

ğŸ‘‰ ì„¤ëª…:
- ì›ë˜ ì…ë ¥ ğ‘‹ì— self-attention ê²°ê³¼ë¥¼ ë”í•¨ (Residual Connection)
- ê·¸ ë‹¤ìŒ, ì „ì²´ ë²¡í„°ë¥¼ ì •ê·œí™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
â†’ í•™ìŠµ ì•ˆì •ì„±, ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ 

ğŸ“Œ í•µì‹¬ ê°œë…:
- ê¸°ì¡´ ì •ë³´ì™€ ìƒˆë¡œ ê³„ì‚°ëœ ì •ë³´ë¥¼ ì„ì€ ë’¤, ê· í˜• ì¡íŒ ë²¡í„°ë¡œ ë‹¤ì‹œ ì •ë¦¬

---

### 5. Feed Forward Network (FFN)

![FFN](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;\text{FFN}(x)%20%3D%20\text{ReLU}(xW_1%20+%20b_1)W_2%20+%20b_2)

ğŸ‘‰ ì„¤ëª…:
- ê° ìœ„ì¹˜ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” 2ì¸µ MLP
- \( W_1, W_2 \): ì„ í˜• ë³€í™˜ ê°€ì¤‘ì¹˜  
- ì¤‘ê°„ì— [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)ë¥¼ ì¨ì„œ ë¹„ì„ í˜•ì„± ë¶€ì—¬

ğŸ“Œ í•µì‹¬ ê°œë…:
- Attentionìœ¼ë¡œ ìš”ì•½ëœ ì •ë³´ë¥¼ í•œ ë²ˆ ë” ìƒê°í•˜ê³  ì •ì œí•˜ëŠ” ì—­í• 

---

### 6. Add & LayerNorm (again)

![AddNorm2](https://latex.codecogs.com/png.image?\fg{gray}\dpi{100}&space;Z_2%20%3D%20\text{LayerNorm}(Z_1%20+%20\text{FFN}(Z_1)))

ğŸ‘‰ ì„¤ëª…:
- FFNì˜ ì¶œë ¥ê³¼ ì´ì „ ë¸”ë¡ì˜ ì¶œë ¥ ğ‘1ì„ ë‹¤ì‹œ ë”í•¨
- ê·¸ í›„ ì •ê·œí™” (LayerNorm)ë¡œ ì•ˆì •ì„± ìœ ì§€

ğŸ“Œ í•µì‹¬ ê°œë…:
- ìƒˆë¡œ ì •ë¦¬ëœ ìƒê°ì„ ì´ì „ ìƒê°ê³¼ ì„ì–´ ì •ëˆí•˜ëŠ” ë§ˆë¬´ë¦¬ ë‹¨ê³„

---

ğŸ§  ì „ì²´ íë¦„ ë‹¤ì‹œ í•œ ë²ˆ
1. ì…ë ¥ ë²¡í„°ë¥¼ Query/Key/Valueë¡œ ë³€í™˜
2. Attentionìœ¼ë¡œ ì„œë¡œë¥¼ ë°”ë¼ë³´ë©° ì¤‘ìš”ë„ë¥¼ í•™ìŠµ
3. ì—¬ëŸ¬ headë¡œ ë‹¤ì–‘í•œ ê´€ì ì„ í•©ì¹¨
4. ê¸°ì¡´ ì •ë³´ì™€ ì„ê³  ì •ë¦¬
5. FFNìœ¼ë¡œ ì¶”ê°€ ì²˜ë¦¬
6. ë‹¤ì‹œ ì„ê³  ì •ë¦¬

---

### ğŸ“š ì¶”ê°€ ì°¸ê³  ìë£Œ

- [Transformerì˜ í° ê·¸ë¦¼ ì´í•´: ê¸°ìˆ ì  ë³µì¡í•¨ ì—†ì´ í•µì‹¬ ì•„ì´ë””ì–´ íŒŒì•…í•˜ê¸°](https://medium.com/@hugmanskj/transformerì˜-í°-ê·¸ë¦¼-ì´í•´-ê¸°ìˆ ì -ë³µì¡í•¨-ì—†ì´-í•µì‹¬-ì•„ì´ë””ì–´-íŒŒì•…í•˜ê¸°-5e182a40459d)
- [PyTorch Transformer Tutorial (Official)](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [Transformer Playground (Visualization Tool)](https://transformer-playground.tensorflow.org/)
