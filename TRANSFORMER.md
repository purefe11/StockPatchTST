# ğŸ¤– Transformer ë‚´ë¶€ êµ¬ì¡°

> í¼ì¦ ì¡°ê° ê°™ì€ ë°ì´í„°ë¥¼ ë³´ê³  ë¯¸ë˜ë¥¼ ìƒìƒí•˜ëŠ” ë˜‘ë˜‘í•œ ë¡œë´‡ ì´ì•¼ê¸°!

---

## ğŸ§© 1. Multi-head Self-Attention  
**â€œê° ì¡°ê°ì´ ë‹¤ë¥¸ ì¡°ê°ë“¤ì„ ë°”ë¼ë³´ë©´ì„œ ì¤‘ìš”ë„ë¥¼ ì •í•œë‹¤.â€**

- í¼ì¦ ì¡°ê°ë“¤(íŒ¨ì¹˜)ë¼ë¦¬ ì„œë¡œë¥¼ ë°”ë¼ë³¸ë‹¤.  
- "ë„ˆëŠ” ë‚˜ì™€ ê´€ë ¨ ìˆì–´!", "ë„Œ ëœ ì¤‘ìš”í•´" ê°™ì€ íŒë‹¨ì„ ì—¬ëŸ¬ ë°©í–¥ì—ì„œ í•œë‹¤.  
- ì—¬ëŸ¬ ê°œì˜ ì‹œì„ ìœ¼ë¡œ ë³¸ë‹¤ê³  í•´ì„œ **Multi-head (ì—¬ëŸ¬ ê°œì˜ ëˆˆ)** ì´ë¼ê³  í•œë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> ì¹œêµ¬ë“¤ê³¼ ëª¨ì—¬ì„œ, "ìš°ë¦¬ ì¤‘ ëˆ„ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì´ì•¼ê¸°ë¥¼ í–ˆëŠ”ì§€" ì—¬ëŸ¬ ì‹œì„ ì—ì„œ ìƒê°í•´ë³´ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤.

---

## â• 2. Add & LayerNorm  
**â€œìì‹ ì˜ ìƒê°ê³¼ ì¹œêµ¬ë“¤ ì˜ê²¬ì„ í•©ì³ì„œ ì •ë¦¬í•œë‹¤.â€**

- ì¹œêµ¬ë“¤ì˜ ì˜ê²¬(Attention ê²°ê³¼)ê³¼ ì›ë˜ ìê¸° ìƒê°ì„ ë”í•œë‹¤.  
- ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ê°€ ë„ˆë¬´ íŠ€ì§€ ì•Šë„ë¡ ì •ë¦¬í•œë‹¤. (= LayerNorm)

ğŸ’¡ *ë¹„ìœ :*  
> ì¹œêµ¬ ì˜ê²¬ + ë‚´ ì˜ê²¬ = ë” ë˜‘ë˜‘í•´ì§„ ë‚˜!

---

## ğŸ”§ 3. Feed Forward (ì„ í˜• â†’ ReLU â†’ ì„ í˜•)  
**â€œìƒê°ì„ ë” ê¹Šê²Œ ì •ë¦¬í•´ì„œ ë‹¤ì‹œ í‘œí˜„í•œë‹¤.â€**

- ë‘ ë²ˆì˜ ê³„ì‚°ì„ í†µí•´ ìƒê°ì„ ë‹¤ë“¬ëŠ”ë‹¤.  
- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)ëŠ” "ìŒìˆ˜ëŠ” ì‹«ì–´!"ë¼ê³  ë§í•˜ë©´ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ë‚¨ê¸´ë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> í•œ ë²ˆ ë” ìƒê°í•´ì„œ ë” ë©‹ì§„ ë¬¸ì¥ì„ ë§Œë“œëŠ” ê³¼ì •ê³¼ ê°™ë‹¤.

---

## â• 4. Add & LayerNorm (ë˜ ë‚˜ì˜¨ë‹¤!)  
**â€œì´ë²ˆì—ë„ ë‚´ ìƒê°ê³¼ ìƒˆë¡œ ì •ë¦¬í•œ ìƒê°ì„ í•©ì³ì„œ ë‹¤ì‹œ ì •ë¦¬í•œë‹¤.â€**

- ì •ë¦¬ëœ ìƒê° + ì›ë˜ ìƒê° â†’ ë‹¤ì‹œ ì •ë¦¬í•´ì„œ ë” ë˜‘ë˜‘í•˜ê²Œ ë§Œë“ ë‹¤.

ğŸ’¡ *ë¹„ìœ :*  
> ë‚´ê°€ í•œ ë§ + ìƒˆë¡œ ì •ë¦¬í•œ ë§ = ì™„ì„±ëœ ë©‹ì§„ ë§!

---

## ğŸ‰ Transformerê°€ ë­˜ í•´ì£¼ëŠ” ê±¸ê¹Œ?

- ì¡°ê°ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì˜ íŒŒì•…í•˜ê³ ,  
- ì–´ë–¤ ì¡°ê°ì´ ì¤‘ìš”í•œì§€ë„ ì•Œì•„ë‚¸ë‹¤.  
- â†’ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ì¤€ë¹„ ì™„ë£Œ!

---

## âœ¨ í•œ ì¤„ ìš”ì•½

> **TransformerëŠ” ì—¬ëŸ¬ ì¡°ê°ì´ ì„œë¡œë¥¼ ë°”ë¼ë³´ë©° ì¤‘ìš”ë„ë¥¼ ì •í•˜ê³ ,  
> ìƒê°ì„ ì •ë¦¬í•´ì„œ ë˜‘ë˜‘í•œ ì˜ˆì¸¡ì„ í•˜ëŠ” êµ¬ì¡°ë‹¤!**

<br>

## ğŸ“š ë” ì•Œì•„ë³´ê¸°

- ["Attention is All You Need"](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)

  ![transformer_resideual_layer_norm_2](https://github.com/user-attachments/assets/7506baf2-33bb-4d5b-a33f-cd4e2f10bae3)
